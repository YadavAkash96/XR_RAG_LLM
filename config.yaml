index:
  strategy: "caption_to_text"   # or "shared_encoder"
  text_model: "sentence-transformers/all-MiniLM-L6-v2"
  caption_model: "nlpconnect/vit-gpt2-image-captioning"
  use_ocr: true
  languages: ["en"]
  min_chunk_tokens: 10
  max_chunk_tokens: 400
  store_images: false
faiss:
  use_ivf_pq: true
  nlist: 2048
  m_pq: 64



# index:
#   strategy: "caption_to_text" # or "shared_encoder"
#   db_dir: data
#   text_model: "BAAI/bge-m3"
#   image_model: "openai/clip-vit-base-patch32"
#   shared_encoder_model: "sentence-transformers/clip-ViT-B-32"  # or "sentence-transformers/siglip-base-patch16-384"
#   # Only used when strategy=caption_to_text:
#   caption_model: "nlpconnect/vit-gpt2-image-captioning"  # light & T4-friendly
#   # Or better (but heavier): "Salesforce/blip-image-captioning-base"
#   use_ocr: true
#   min_chunk_tokens: 10
#   max_chunk_tokens: 500
#   languages: ["en"]
#   store_images: true


faiss:
  use_ivf_pq: true
  nlist: 2048     # tune to corpus size
  m_pq: 64        # product quantization codebooks
  nprobe: 16      # query-time probes

retrieval:
  top_k_text: 40
  top_k_image: 12
  fuse_weight_text: 0.65
  fuse_weight_image: 0.35
  use_reranker: true
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

llm:
  provider: "huggingface" #"ollama"
  model: "HuggingFaceH4/zephyr-7b-alpha"  # meta-llama/Llama-2-7b-hf , HuggingFaceH4/zephyr-7b-alpha, mistralai/Mistral-7B-Instruct-v0.2
  token: "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"
  quantization: "8bit"
  temperature: 0.2
  max_tokens: 600

answers:
  require_citations: true
  output_format: "json"

tts:
  provider: "coqui"            # Local TTS, avoid cloud latency
  voice: "alloy"

device:
  use_cuda: True               # Auto-detect GPU
  device_name: "cuda"



