index:
  strategy: "caption_to_text"   # or "shared_encoder"
  text_model: "sentence-transformers/all-MiniLM-L6-v2"
  caption_model: "nlpconnect/vit-gpt2-image-captioning"
  use_ocr: true
  languages: ["en"]
  min_chunk_tokens: 10
  max_chunk_tokens: 400
  store_images: false
faiss:
  use_ivf_pq: true
  nlist: 2048
  m_pq: 64


faiss:
  use_ivf_pq: true
  nlist: 2048     # tune to corpus size
  m_pq: 64        # product quantization codebooks
  nprobe: 16      # query-time probes

retrieval:
  top_k_text: 40
  top_k_image: 12
  fuse_weight_text: 0.65
  fuse_weight_image: 0.35
  use_reranker: true
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

llm:
  provider: "huggingface" #"ollama"
  model: "HuggingFaceH4/zephyr-7b-alpha"  # meta-llama/Llama-2-7b-hf , HuggingFaceH4/zephyr-7b-alpha, mistralai/Mistral-7B-Instruct-v0.2
  token: "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"
  quantization: "8bit"
  temperature: 0.2
  max_tokens: 600

answers:
  require_citations: true
  output_format: "json"

tts:
  provider: "coqui"            # Local TTS, avoid cloud latency
  voice: "alloy"

device:
  use_cuda: True               # Auto-detect GPU
  device_name: "cuda"

# In: config.yaml
services:
  STT_ENDPOINT: "http://localhost:5002/transcribe"      # Update with the STT endpoint from whisper_server.py
  LLM_ENDPOINT: "http://localhost:8001/query"      # Update with the RAG + LLM endpoint from RAG_LLM/app.py